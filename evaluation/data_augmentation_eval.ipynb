{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "# machine learning\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# signal processing\n",
    "from scipy import signal\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wifi-staff-172-24-24-22.net.auckland.ac.nz\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "myHostName = socket.gethostname()\n",
    "print(myHostName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "# number of signals per iteration\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWDataset(Dataset):\n",
    "    def __init__(self, signals_csv, parameters_csv, parameter):\n",
    "        self.parameters = pd.read_csv(parameters_csv)\n",
    "        self.signals = pd.read_csv(signals_csv).astype('float32')\n",
    "\n",
    "        # remove unusual parameters and corresponding signals\n",
    "        keep_signals_idx = np.array(self.parameters[self.parameters['beta1_IC_b'] > 0].index)\n",
    "        self.parameters = self.parameters.iloc[keep_signals_idx,:]\n",
    "        self.signals = self.signals.iloc[:,keep_signals_idx]\n",
    "\n",
    "        # process beta_ic_b parameter\n",
    "        ranges = [0, 0.06, 0.17, 1]\n",
    "        labels = [0, 1, 2]\n",
    "        num_classes = len(labels)\n",
    "        self.beta = self.parameters['beta1_IC_b']\n",
    "        self.beta = pd.cut(self.beta, bins=ranges, labels=labels).astype('int')\n",
    "        self.beta = self.beta.values\n",
    "        self.beta = np.eye(num_classes)[self.beta]\n",
    "        self.beta = np.reshape(self.beta, (self.beta.shape[0], self.beta.shape[1], 1)).astype('float32')\n",
    "\n",
    "        # bin selected labels according to literature or equal-freq binning where appropriate\n",
    "        if (parameter == 'beta1_IC_b'):  \n",
    "            self.parameters = self.beta\n",
    "        elif (parameter == 'A(km)'):\n",
    "            self.parameters = self.parameters[parameter]\n",
    "            num_classes = 5\n",
    "            replacement = {300: 0, 467: 1, 634: 2, 1268: 3, 10000: 4}\n",
    "            self.parameters = self.parameters.replace(replacement)\n",
    "            self.parameters = self.parameters.values\n",
    "            self.parameters = np.eye(num_classes)[self.parameters]\n",
    "        elif (parameter == 'omega_0(rad|s)'):\n",
    "            self.parameters = self.parameters[parameter]\n",
    "            num_classes = 5\n",
    "            dummy, bin_boundaries = pd.qcut(self.parameters, q= num_classes, retbins=True, duplicates='drop')\n",
    "            print(bin_boundaries)\n",
    "            self.parameters = self.parameters.values\n",
    "            self.parameters = np.digitize(self.parameters,bins=bin_boundaries[1:num_classes])\n",
    "            y = np.eye(num_classes)[y]\n",
    "        elif (parameter == 'Ye_c_b'):\n",
    "            self.parameters = self.parameters[parameter]\n",
    "            num_classes = 5\n",
    "            dummy, bin_boundaries = pd.qcut(self.parameters, q=num_classes, retbins=True, duplicates='drop')\n",
    "            print(bin_boundaries)\n",
    "            self.parameters = self.parameters.values\n",
    "            self.parameters = np.digitize(self.parameters,bins=bin_boundaries[1:num_classes])\n",
    "            self.parameters = np.eye(num_classes)[self.parameters]\n",
    "        \n",
    "\n",
    "        self.original_parameters = self.parameters\n",
    "        self.augmented_parameters = np.empty(shape = (0, self.parameters.shape[1])).astype('float32')\n",
    "\n",
    "        self.signals = self.signals.values\n",
    "        self.original_signals = self.signals\n",
    "        self.augmented_signals = np.empty(shape = (256, 0)).astype('float32')\n",
    "\n",
    "        ### signal manipulation section ###        \n",
    "        temp_signals = np.empty(shape = (256, 0)).astype('float32')\n",
    "        for i in range(0, self.signals.shape[1]):\n",
    "            signal = self.signals[:, i]\n",
    "            signal = signal.reshape(1, -1)\n",
    "\n",
    "            cut_signal = signal[:, int(len(signal[0]) - 256):len(signal[0])]\n",
    "\n",
    "            temp_signals = np.insert(temp_signals, temp_signals.shape[1], cut_signal, axis=1)\n",
    "\n",
    "        \n",
    "        self.signals = temp_signals\n",
    "        ### end signal manipulation section ###\n",
    "\n",
    "    ### augmentation methods ###\n",
    "    def jittering_augmentation(self, signal):\n",
    "        noise = np.random.normal(0, 1, signal.shape[1])\n",
    "        jittered_signal = signal + noise\n",
    "        return jittered_signal\n",
    "    \n",
    "    def shift_augmentation(self, signal):\n",
    "        shift = np.random.normal(0, 50, 1)\n",
    "        shifted_signal = np.roll(signal, int(shift))\n",
    "        return shifted_signal\n",
    "\n",
    "    def scale_augmentation(self, signal):\n",
    "        scale_factor = np.random.normal(1, 0.5, 1)\n",
    "        scale_factor = np.maximum(scale_factor, 0)\n",
    "        scaled_signal = scale_factor * signal\n",
    "        return scaled_signal\n",
    "\n",
    "    def mixture_augmentation(self, signal_1, signal_2):\n",
    "        distance_multiplier = np.random.normal(0.5, 0.2, 1)\n",
    "        # clip signal to range [0,1] as this is the multiplier by the normalised difference in signals\n",
    "        distance_multiplier = np.clip(distance_multiplier, 0, 1)\n",
    "        mixture_signal = signal_1 + distance_multiplier * (signal_2 - signal_1)\n",
    "        return mixture_signal\n",
    "\n",
    "    def window_warping_augmentation(self, signal):\n",
    "        # take window size of 10% of the signal with a warping factor of 2 or 0.5 (from literature)\n",
    "        warping_factor =  np.random.choice([0.5, 2])\n",
    "        # warping_factor = 0.5\n",
    "\n",
    "        window_size = math.floor(signal.shape[1] / 10)\n",
    "        scaled_window_size = warping_factor * window_size\n",
    "\n",
    "        # don't warp anything a little bit before the core-bounce - preserves core-bounce position\n",
    "        window_min_idx = 53\n",
    "\n",
    "        # find random reference position for start of window warping\n",
    "        window_start_idx = np.random.randint(window_min_idx, signal.shape[1] - scaled_window_size*2)\n",
    "        window_end_idx = window_start_idx + window_size\n",
    "\n",
    "        # select between warping by factor 1/2 or 2\n",
    "        if (warping_factor == 2):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:int(signal.shape[1]-(window_size))]\n",
    "\n",
    "            # time points\n",
    "            t = np.arange(len(signal_window))\n",
    "            warped_t = np.arange(0, len(signal_window), 0.5)\n",
    "\n",
    "            # interpolation for window warping\n",
    "            signal_window_warped = np.interp(warped_t, t, signal_window)\n",
    "\n",
    "            # combine signals\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        elif (warping_factor == 0.5):\n",
    "            # extract values before, at and after the window\n",
    "            # clip end of signal to make up for extra size due to window warping\n",
    "            signal_before_window = signal[0][:window_start_idx]\n",
    "            signal_window = signal[0][window_start_idx:window_end_idx]\n",
    "            signal_after_window = signal[0][window_end_idx:]\n",
    "            # add values to end of signal to make up for downsampled window\n",
    "            signal_after_window = np.pad(signal_after_window, (0, int(window_size - scaled_window_size)), mode='edge')\n",
    "\n",
    "            signal_window_warped = signal_window[::int(1/warping_factor)]\n",
    "\n",
    "            warped_signal = np.concatenate((signal_before_window, signal_window_warped, signal_after_window), axis=0)\n",
    "        else:\n",
    "            warped_signal = signal\n",
    "        return warped_signal\n",
    "\n",
    "    def augmentation(self, desired_augmented_data_count):\n",
    "        while self.signals.shape[1] < desired_augmented_data_count:\n",
    "            idx_1 = np.random.randint(0, 1684)\n",
    "            signal_1 = self.signals[:, idx_1]\n",
    "            signal_1 = signal_1.reshape(1, -1)\n",
    "\n",
    "            ### mixture augmentation only ###\n",
    "            # find the class of signal_1 (assuming class is a column in self.parameters)\n",
    "            beta_class_of_signal_1 = np.argmax(self.beta[idx_1, :])\n",
    "            # sample only from the same class for signal_2 and make sure it's not the same as signal_1\n",
    "            candidate_indices = [x for x in range(0, 1684) if x != idx_1 and np.argmax(self.beta[x, :]) == beta_class_of_signal_1]\n",
    "            idx_2 = np.random.choice(candidate_indices)\n",
    "            signal_2 = self.signals[:, idx_2]\n",
    "            signal_2 = signal_2.reshape(1, -1)\n",
    "\n",
    "            # call selected augmentation function here\n",
    "            # augmented_signal = self.window_warping_augmentation(signal_1)\n",
    "            augmented_signal = self.mixture_augmentation(signal_1, signal_2)\n",
    "\n",
    "            self.augmented_signals = np.insert(self.augmented_signals, self.augmented_signals.shape[1], augmented_signal, axis=1)\n",
    "            self.signals = np.insert(self.signals, self.signals.shape[1], augmented_signal, axis=1)\n",
    "\n",
    "            # just copy parameters for now\n",
    "            augmented_parameter = self.parameters[idx_1, :]\n",
    "\n",
    "            self.augmented_parameters = np.insert(self.augmented_parameters, self.augmented_parameters.shape[0], augmented_parameter, axis=0)\n",
    "            self.parameters = np.insert(self.parameters, self.parameters.shape[0], augmented_parameter, axis=0)\n",
    "\n",
    "        print(\"Signal Dataset Size after Data Augmentation: \", self.signals.shape)\n",
    "        print(\"Parameter Dataset Size after Data Augmentation: \", self.parameters.shape)\n",
    "\n",
    "    ### overloads ###\n",
    "    def __len__(self):\n",
    "        return self.signals.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.signals[:, idx]\n",
    "        signal = signal.reshape(1, -1)\n",
    "\n",
    "        parameter = self.parameters[idx,:]\n",
    "        parameter = parameter.reshape(1, -1)\n",
    "\n",
    "        return signal, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]]]\n",
      "(1684, 3, 1)\n",
      "Signal Dataset Size after Data Augmentation:  (256, 6000)\n",
      "Parameter Dataset Size after Data Augmentation:  (6000, 5)\n"
     ]
    }
   ],
   "source": [
    "parameter = 'A(km)'\n",
    "dataset = GWDataset(\"../data/richers_1764.csv\", \"../data/richers_1764_parameters.csv\", parameter)\n",
    "dataset.augmentation(6000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
